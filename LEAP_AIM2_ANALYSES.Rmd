---
title: "LEAP_AIM2 ANALYSES"
author: "Meriah DeJoseph"
date: '2022 - 2023'
output: html_document
---

## About
This file generates alpha values from DFA for physio data for each person and then merges them into one dataset. This dataset is then merged with the comp parameters from Aim 1 and multilevel brms models are conducted to examine substantive questions.

DFA pulled from this tutorial: https://rhrv.r-forge.r-project.org/tutorial/tutorial.pdf

## Libraries & working directory
```{r, warning=FALSE, echo=TRUE, include=TRUE, message=FALSE}

library('dplyr')
library('tidyr')
library('readr')
library('hBayesDM')
library('brms')
library('knitr')
library('vioplot')
library('patchwork')
library('RColorBrewer')
library('stats')
library('ggplot2')
library('ggmap')
library('AER')
library('glmnet')
library('igraph')
library('xtable')
library('corrplot')
library('ggdensity')
library('ggpubr')
library('loo')
library('lme4')
library('broom')
library('interactions')
library('RHRV')
library('stringr')
library('gptstudio')
library('purrr')
library('bayesAB')

setwd("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/")

```

## Set up batching process
```{r}

# create a vector of folder names
folders <- sprintf("%03d_edited", 1:67)

# iterate over the folders
for (folder in folders) {
  # create the path to the folder
  path <- file.path("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/", folder)
  
  # get the list of CSV files in the folder
  files <- list.files(path, pattern = "\\.csv$", full.names = TRUE)
  
  # iterate over the CSV files
  for (file in files) {
    dat <- read.csv(file, header = FALSE)
    dat <- dat[, -c(1, 3:4)]  # remove columns 1 and 3:4
    newname <- sub("\\.csv", ".txt", basename(file))
    # write the resulting data to the folder
    write.table(dat, file.path(path, newname), sep="\t", row.names = FALSE, col.names = FALSE)
  }
}

```

## Descriptives on lengths of time series
```{r}

# create a vector of folder names
folders <- sprintf("%03d_edited", 1:67)

# initialize a vector to store the number of rows
rows <- c()

# iterate over the folders
for (folder in folders) {
  # create the path to the folder
  path <- file.path("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/", folder)
  
  # get the list of TXT files in the folder
  files <- list.files(path, pattern = "\\.txt$", full.names = TRUE)
  
  # iterate over the TXT files
  for (file in files) {
    # read in the data from the TXT file
    dat <- read.table(file, header = FALSE)
    
    # store the number of rows in the vector
    rows <- c(rows, nrow(dat))
  }
}

# calculate the mean, sd, and range of the number of rows
mean_rows <- mean(rows)
sd_rows <- sd(rows)
range_rows <- range(rows)

# print the results
cat("Mean number of rows across all files:", mean_rows, "\n")
cat("Standard deviation of number of rows across all files:", sd_rows, "\n")
cat("Range of number of rows across all files:", range_rows[1], "-", range_rows[2], "\n")

#descriptives on all data:
# > cat("Mean number of rows across all files:", mean_rows, "\n")
# Mean number of rows across all files: 345.2286 
# > cat("Standard deviation of number of rows across all files:", sd_rows, "\n")
# Standard deviation of number of rows across all files: 69.19665 
# > cat("Range of number of rows across all files:", range_rows[1], "-", range_rows[2], "\n")
# Range of number of rows across all files: 177 - 817 

#177 is a bit small. let's see how many files are less than 200 


# create a vector of folder names
folders <- sprintf("%03d_edited", 1:67)

# iterate over the folders
for (folder in folders) {
  # create the path to the folder
  path <- file.path("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/", folder)
  
  # get the list of TXT files in the folder
  files <- list.files(path, pattern = "\\.txt$", full.names = TRUE)
  
  # iterate over the TXT files
  for (file in files) {
    # read in the data from the TXT file
    dat <- read.table(file, header = FALSE)
    
    # check if the number of rows is less than 200
    if (nrow(dat) < 200) {
      # print the name of the file
      cat("File", basename(file), "has less than 200 rows.\n")
    }
  }
}

#files below had less than 200. im going to remove these and set to missing
# File LEAP_012_20220430_SK_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_017_20220515_MLD_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_033_20220612_AS_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_054_20220618_SK_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_058_20220723_AS_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_061_20220714_CD_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.
# File LEAP_063_20220709_CD_3_19_Youth ECG - ECG_IBI_Edited.txt has less than 200 rows.

#removed these manually and added them to folder called "IBI with less than 200"




#############
#now re-do descriptives with included data:

# create a vector of folder names
folders <- sprintf("%03d_edited", 1:67)

# initialize a vector to store the number of rows
rows <- c()

# iterate over the folders
for (folder in folders) {
  # create the path to the folder
  path <- file.path("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/", folder)
  
  # get the list of TXT files in the folder
  files <- list.files(path, pattern = "\\.txt$", full.names = TRUE)
  
  # iterate over the TXT files
  for (file in files) {
    # read in the data from the TXT file
    dat <- read.table(file, header = FALSE)
    
    # store the number of rows in the vector
    rows <- c(rows, nrow(dat))
  }
}

# calculate the mean, sd, and range of the number of rows
mean_rows <- mean(rows)
sd_rows <- sd(rows)
range_rows <- range(rows)

# print the results
cat("Mean number of rows across all files:", mean_rows, "\n")
cat("Standard deviation of number of rows across all files:", sd_rows, "\n")
cat("Range of number of rows across all files:", range_rows[1], "-", range_rows[2], "\n")

# > cat("Mean number of rows across all files:", mean_rows, "\n")
# Mean number of rows across all files: 347.5404 
# > cat("Standard deviation of number of rows across all files:", sd_rows, "\n")
# Standard deviation of number of rows across all files: 66.94827 
# > cat("Range of number of rows across all files:", range_rows[1], "-", range_rows[2], "\n")
# Range of number of rows across all files: 201 - 817 

```


Notes for setting DFA parameters below; to discuss with Dan:

indexNonLinearAnalysis = 1 
-indicates a polynomial detrending order of m=1

windowSizeRange = c(10,50)
- From Dan paper: The range of window sizes is intended to capture the range of temporal resolution that is meaningful for the system (Ihlen, 2012). For instance, with a common HRV times series length of ~ 1000 points, a typical range of window sizes might be as small as 4 adjacent heartbeats and as large as a quarter of the size of the entire series (i.e., 250). 
- So I guess since my series here is 364 points, then I'll use a min of 4 and a max of 91 (i.e. a quarter of 364)? Still not clear on npoints
- From Isa diss: Infant IBIs had a mean length of 2670, median length of 2478 with a standard deviation of 936, ranging from 948 to 5286. As per recommendations from Ihlen (2012) and previous work, the following parameters were defined: polynomial detrending order m=1 meaning that linear detrending was used, minimum window size scmin=16 or the minimum segment size, maximum window size scmax= length(time-series)/10 or the maximal segment size
- From Robin's diss: (time series: average length of 1,685 samples (min=1,000, max=2,800)) Mean α estimates were stable once the smallest segment size (scmin) was increased from 4 to 8; doing so decreased the average α estimates by 0.05, or 0.46 SDs. This corroborates past recommendations which note that when the minimum segment size is too small, α estimates get inflated (Ihlen, 2012). When the maximum segment size is too small α – as was the case for shorter fixations where scmax was set to length/10 rather than length/4) – estimates are more variable (and in rare cases have negative values). This parameter did not meaningfully impact R2 estimates. Given these visualizations, I will continue using scmax=length/4. 
-basically chunks of the profile to calc sd from? take diff chunks as small as the min and as big as the max
- default for package is 10-300
-Ihlen: The statistical argument is to choose minimum and maximum segment sizes that provide a numerical stable estimation of RMS and Fq in Matlab code 8 and 12. The minimum segment sample size should be large enough to prevent error in the computation of local fluctuation RMS. The minimum segment size larger than 10 samples is a “rule of tumb” for the computation of RMS.In MFDFA1, the maximum segment size should be small enough to provide a suf- sufficient number of segments in the computation of Fq in Matlab code 8. A maximum segment size below 1/10 of the sample size of the time series will provide at least 10 segments in the computation of Fq in Matlab code 8. Furthermore, it’s favorable to have a equal spacing between scales when
-FINAL DECISION: 10 min and 4th of total size; here minimum time series is 200, so will use 10 min and 50 max

npoints = 10
- number of different window sizes to compute within the range
- Isa's diss: scres=19 or the total number of segment sizes.
- Robin's diss: increasing the total number of window sizes at which RMS is calculated(scres) did not have a meaningful impact on α or R2 estimates. However, given that there is no reason to not increase this value other than increased computational burden, I doubled the number of window sizes from 4 to 8
- FINAL DECISION: quarter of poss window sizes; here possible sizes are 40, so a quarter is 10

regressionRange = c(4, 64)
- From package doc: In order to obtain a estimate of some scaling exponent, the user must use the EstimateDFA function specifying the regression range (window sizes used to detrend the series). α1 is usually obtained by performing the regression in the 4 ≤ t ≤ 16 range whereas that α2 is obtained in the 16 ≤ t ≤ 64 range (However the F(t) function must be linear in these ranges to obtain reliable results).
-slow vs long range capturing memory of the system?
- From Ihlen 2012: The slope, H, of the regression line, RegLine, is called the Hurst exponent (Hurst, 1951). The Hurst exponent defines the monofractal structure of the time series by how fast the overall RMS,F, of local fluctuations, RMS, grows with increasing segment sample size. The larger Hurst exponent,H, is visually seen as more slowevolving variations (i.e., more persistent structure) inmonfractalandmultifractaltime series compared with whitenoise.
-FINAL DECISION: 4 to 64 to capture full range

SENSITIVITY checks for later:
- decide parameters based on smallest time series
- sens by 10, 15, 20 min
- long vs short range
- pull pics of linear relations 

## DFA loop through all cleaned IBI files and save out alpha
```{r}

#side note: to run on one file, put this line under the loadascii file
##############
#comment the line below out when done testing
# hrv.data <- CreateHRVData()
# hrv.data <- LoadBeatAscii(hrv.data,"LEAP_059_20220709_SB_6_22_Youth ECG - ECG_IBI_Edited.txt", RecordPath = ".")
PlotNIHR(hrv.data)
################


# Set up the ID and segment extractor regexes
id_regex <- "LEAP_(\\d+)"
segment_regex <- "_(\\d+)_Youth"

# create a vector of folder names
folders <- sprintf("%03d_edited", 1:67)

# iterate over the folders
for (folder in folders) {
  # create the path to the folder
  path <- file.path("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES/", folder)
  
  # get the list of TXT files in the folder
  files <- list.files(path, pattern = "\\.txt$", full.names = TRUE)
  
  # Loop through each file
  for (file in files) {
    # Extract the ID and segment from the file name
    id <- str_extract(basename(file), id_regex)
    segment <- str_extract(basename(file), segment_regex)

    # Create the HRV data structure
    hrv.data <- CreateHRVData()

    # Load the IBI data from the file
    hrv.data <- LoadBeatAscii(hrv.data, basename(file), RecordPath = path)

    # Build the heart beats
    hrv.data <- BuildNIHR(hrv.data)

    # Create the data structure for nonlinear analysis
    hrv.data <- CreateNonLinearAnalysis(hrv.data)
    hrv.data <- SetVerbose(hrv.data, TRUE )

    # Calculate DFA
    hrv.data <- CalculateDFA(hrv.data, indexNonLinearAnalysis = 1,
                             windowSizeRange = c(10, 50), npoints = 10, doPlot = TRUE)

    # Estimate DFA
    hrv.data <- EstimateDFA(hrv.data, indexNonLinearAnalysis = 1,
                           regressionRange = c(4,64), doPlot = TRUE)

    # Extract the scaling exponent value
    scaling.exp <- hrv.data$NonLinearAnalysis[[1]]$dfa$statistic[[1]]
    scaling.exp <- scaling.exp$estimate[[1]]

    # Create a data frame with the ID, segment, and scaling exponent
    results <- data.frame(ID = id, Segment = segment, Scaling.Exponent = scaling.exp)

    # Write the results to a .csv file with the modified file name
    newname <- sub("\\.txt", "_ALPHA.csv", basename(file))
    write.csv(results, file = file.path(path, newname), row.names = FALSE)
  }
}

```











## Clean and merge DFA output back in with accparsmerged and accparslong created in aim 1
```{r}

#merge raw output in long form, with stacked ID, segment, and alpha
# Create an empty list to store the dataframes
df_list <- list()

# Loop through the 67 folders
for (i in 1:67) {
  # Read all the ALPHA files in the folder
  df_list[[i]] <- list.files(paste0(sprintf("%03d", i), "_edited"), pattern="_ALPHA\\.csv$", full.names=TRUE) %>% 
    map_dfr(~read.table(.x, sep=',', header=TRUE, stringsAsFactors=FALSE))
}

# Bind all the dataframes in the list vertically
rawDFAmerged <- bind_rows(df_list)

# Print and save out the raw dataframe
print(rawDFAmerged)
write.csv(rawDFAmerged, "rawDFAmerged.csv")






######################################
########merge in raw dataset and clean it up
DFAmerged <- read.csv("rawDFAmerged.csv")
DFAmerged$X <- NULL

#clean up the vars
# Remove "LEAP_" from the 'ID' column
DFAmerged$ID <- gsub("LEAP_", "", DFAmerged$ID)

# Replace "<NA>" with "1" in the 'Segment' column
DFAmerged$Segment[is.na(DFAmerged$Segment)] <- "1"

# Remove everything except the number from the 'Segment' column
DFAmerged$Segment <- gsub("[^[:digit:]]", "", DFAmerged$Segment)

#change ID to numeric
DFAmerged$ID <- as.numeric(DFAmerged$ID)

# Print the final dataframe
print(DFAmerged)



#make original labels
DFAmerged <- DFAmerged %>% 
  mutate(origlabel = case_when(
   Segment == "1" ~ "Baseline",
   Segment == "18" ~ "AcqRun1",
   Segment ==  "19" ~ "AcqRun2",
   Segment ==  "20" ~ "AcqRun3",
   Segment ==  "21" ~ "AcqRun4",
   Segment ==  "22" ~ "RevRun1",
   Segment ==  "23" ~ "RevRun2",
   Segment ==  "24" ~ "RevRun3",
   Segment ==  "25" ~ "RevRun4"
  ))

#put in order of presentation to sort by later
#this is the order it was actually presented
DFAmerged <- DFAmerged %>% 
  mutate(origorder = case_when(
   origlabel == "Baseline" ~ "0",
   origlabel == "AcqRun1" ~ "1",
   origlabel == "AcqRun2" ~ "3",
   origlabel == "AcqRun3" ~ "5",
   origlabel == "AcqRun4" ~ "7",
   origlabel == "RevRun1" ~ "2",
   origlabel == "RevRun2" ~ "4",
   origlabel == "RevRun3" ~ "6",
   origlabel == "RevRun4" ~ "8"
  ))







#create variable that denotes which version came first, where 1=pics first and 2=lines first (copied directly from version order checking file)
DFAmerged <- DFAmerged %>%
  group_by(ID) %>%  # Group the data by ID
  mutate(versionorder = case_when(
ID	==	001	~	1	,
ID	==	002	~	2	,
ID	==	003	~	2	,
ID	==	004	~	2	,
ID	==	005	~	1	,
ID	==	006	~	2	,
ID	==	007	~	1	,
ID	==	008	~	2	,
ID	==	010	~	2	,
ID	==	012	~	2	,
ID	==	014	~	2	,
ID	==	015	~	1	,
ID	==	016	~	2	,
ID	==	017	~	1	,
ID	==	018	~	2	,
ID	==	019	~	1	,
ID	==	021	~	2	,
ID	==	022	~	2	,
ID	==	023	~	1	,
ID	==	024	~	2	,
ID	==	025	~	1	,
ID	==	026	~	2	,
ID	==	027	~	1	,
ID	==	028	~	2	,
ID	==	029	~	1	,
ID	==	030	~	2	,
ID	==	031	~	1	,
ID	==	032	~	2	,
ID	==	033	~	1	,
ID	==	034	~	2	,
ID	==	036	~	2	,
ID	==	037	~	1	,
ID	==	038	~	2	,
ID	==	039	~	1	,
ID	==	040	~	2	,
ID	==	042	~	2	,
ID	==	043	~	1	,
ID	==	044	~	2	,
ID	==	046	~	2	,
ID	==	047	~	1	,
ID	==	048	~	2	,
ID	==	049	~	1	,
ID	==	050	~	2	,
ID	==	051	~	1	,
ID	==	052	~	2	,
ID	==	053	~	1	,
ID	==	054	~	2	,
ID	==	055	~	1	,
ID	==	056	~	2	,
ID	==	058	~	2	,
ID	==	059	~	1	,
ID	==	061	~	1	,
ID	==	062	~	2	,
ID	==	063	~	1	,
ID	==	064	~	2	,
ID	==	067	~	1	,
  ))

#create time-varying index variable so that baseline, then lines, then pics alpha scores can be organized to represent a continuous flow of change across versions and presentation of the runs; 1=pics first and 2=lines
#ppl who got pics first would have had baseline, then acq/rev1, acq/rev2, acq/rev3, acq/rev4
#ppl who got lines first would have had baseline, then acq/rev4, acq/rev3, acq/rev2, acq/rev1
DFAmerged <- DFAmerged %>% 
  mutate(versrunlabel = case_when(
    versionorder == 1 & origlabel == "Baseline" ~ "Baseline",
    versionorder == 1 & origlabel ==  "AcqRun1" ~ "picsAcqRun1",
    versionorder == 1 & origlabel ==  "RevRun1" ~ "picsRevRun1",
    versionorder == 1 & origlabel ==  "AcqRun2" ~ "picsAcqRun2",
    versionorder == 1 & origlabel == "RevRun2" ~ "picsRevRun2",
    versionorder == 1 & origlabel ==  "AcqRun3" ~ "linesAcqRun1",
    versionorder == 1 & origlabel ==  "RevRun3" ~ "linesRevRun1",
    versionorder == 1 & origlabel ==  "AcqRun4" ~ "linesAcqRun2",
    versionorder == 1 & origlabel ==  "RevRun4" ~ "linesRevRun2",
  
    versionorder == 2 & origlabel == "Baseline" ~ "Baseline",
    versionorder == 2 & origlabel ==  "AcqRun1" ~ "linesAcqRun1",
    versionorder == 2 & origlabel ==   "RevRun1" ~ "linesRevRun1",
    versionorder == 2 & origlabel ==   "AcqRun2" ~ "linesAcqRun2",
    versionorder == 2 & origlabel ==   "RevRun2" ~ "linesRevRun2",
    versionorder == 2 & origlabel ==   "AcqRun3" ~ "picsAcqRun1",
    versionorder == 2 & origlabel ==   "RevRun3" ~ "picsRevRun1",
    versionorder == 2 & origlabel ==   "AcqRun4" ~ "picsAcqRun2",
    versionorder == 2 & origlabel ==   "RevRun4" ~ "picsRevRun2"
  ))

#make two versions of numeric versrunlabel numeric, with runs repeating for version separately (1-4 for each version), and then combined (1-8 with lines first)
DFAmerged <- DFAmerged %>% 
  mutate(versordernum2 = case_when(
    versrunlabel == "Baseline" ~ 0,
    versrunlabel == "linesAcqRun1" ~ 1,
    versrunlabel == "linesRevRun1" ~ 2,
    versrunlabel == "linesAcqRun2" ~ 3,
    versrunlabel == "linesRevRun2" ~ 4,
    versrunlabel == "picsAcqRun1" ~ 1,
    versrunlabel == "picsRevRun1" ~ 2,
    versrunlabel == "picsAcqRun2" ~ 3,
    versrunlabel == "picsRevRun2" ~ 4
  ))

DFAmerged <- DFAmerged %>% 
  mutate(versordernum1 = case_when(
    versrunlabel == "Baseline" ~ 0,
    versrunlabel == "linesAcqRun1" ~ 1,
    versrunlabel == "linesRevRun1" ~ 2,
    versrunlabel == "linesAcqRun2" ~ 3,
    versrunlabel == "linesRevRun2" ~ 4,
    versrunlabel == "picsAcqRun1" ~ 5,
    versrunlabel == "picsRevRun1" ~ 6,
    versrunlabel == "picsAcqRun2" ~ 7,
    versrunlabel == "picsRevRun2" ~ 8
  ))


#make var that denotes which version to filter on later
DFAmerged <- DFAmerged %>% 
  mutate(version = case_when(
    versordernum1 == 1 ~ "lines",
    versordernum1 == 2 ~ "lines",
    versordernum1 == 3 ~ "lines",
    versordernum1 == 4 ~ "lines",
    versordernum1 == 5 ~ "pics",
    versordernum1 == 6 ~ "pics",
    versordernum1 == 7 ~ "pics",
    versordernum1 == 8 ~ "pics",
  ))

#add time invariant baseline value if needed for covariate
DFAmerged <- DFAmerged %>%
  group_by(ID) %>%  # Group the data by ID
  mutate(basealpha = Scaling.Exponent[Segment == 1]) %>%  # Select alpha for the first segment
  ungroup()  # Remove grouping


#mutate new alphaLines and alphaPics variables by averaging across runs for each version
DFAmerged <- DFAmerged %>%
  group_by(ID, version) %>% 
  mutate(avgalpha = mean(Scaling.Exponent)) %>% 
  ungroup() 





#save out long/wide DFA data and save out as dfalongmerged
write.csv(DFAmerged, "dfalongmerged.csv")

#make wide version of DFA data and save out as dfawidemerged
#just need to filter by version and pull one of the runs since it repeats across runs for the summary scores
linesDFA <- DFAlongmerged %>%
  filter(version=="lines") %>% 
  filter(versrunlabel == "linesAcqRun1") %>% 
  select(ID, version, basealpha, avgalpha) %>% 
  rename(Subject = ID)

picsDFA <- DFAlongmerged %>%
  filter(version=="pics") %>% 
  filter(versrunlabel == "picsAcqRun1") %>% 
  select(ID, version, basealpha, avgalpha) %>% 
  rename(Subject = ID)
#ID #3 is missing pics and was missing its baseline alpha, so I need to add an ID row for them with blank values for base and alpha
# Create a new data frame with the values that you want to add to picsDFA
new_row = data.frame(Subject = 003, version = "pics", basealpha = 0.753860461, avgalpha = NA)
# Bind the new row to the bottom of the existing data frame
picsDFA = rbind(picsDFA, new_row)

#save out individual wide datasets
write.csv(linesDFA, "lineswideDFA.csv")
write.csv(picsDFA, "picswideDFA.csv")


#merge the two time point data 
dfalinespicsavg <- rbind(linesDFA, picsDFA, by = "Subject")
dfalinespicsavg <- subset(dfalinespicsavg , Subject != "Subject")

#merge the above with accparslong
accparslong <- read.csv("accparslong.csv")
accparslong$X <- NULL
accparslong<- accparslong[order(accparslong$Subject, accparslong$version),]

accparsdfalong <- merge(accparslong, dfalinespicsavg, by = c("Subject", "version"), all=T)
#save out
write.csv(accparsdfalong, "accparsdfalong.csv")


#wide data and merge with accMerged
accparsmerged <- read.csv("accparsmerged.csv")
accparsmerged$X <- NULL

linesDFA2 <- linesDFA %>% 
  select(Subject, basealpha, avgalpha) %>% 
  rename(linesavgalpha = avgalpha)

picsDFA2 <- picsDFA %>% 
  select(Subject, avgalpha) %>% 
  rename(picsavgalpha = avgalpha)

linespicsDFA2 <- merge(linesDFA2, picsDFA2, by = c("Subject"), all = T)

accparsdfamerged <- merge(accparsmerged, linespicsDFA2, by = c("Subject"), all = T)
#save out
write.csv(accparsdfamerged, "accparsdfamerged.csv")


#now make a long dataset from accparslong and DFAlongmerged
#first filter by version and change ID and select vars
DFAlongmerged2 <- DFAlongmerged %>%
  filter(version=="lines" | version=="pics") 

DFAlongmerged3 <- DFAlongmerged2 %>% 
  rename (Subject = ID) %>% 
  select(Subject, version, versionorder, origlabel, versrunlabel, versordernum2, Scaling.Exponent)

#also change the versordernum2 to have a meaningful zero for modeling
DFAlongmerged3 <- DFAlongmerged3 %>% 
  mutate(versordernum2c = case_when(
    versordernum2 == 1 ~ 0,
    versordernum2 == 2 ~ 1,
    versordernum2 == 3 ~ 2,
    versordernum2 == 4 ~ 3,
  ))

dfaALLpluspars <- merge(DFAlongmerged3, accparslong, by=c("Subject", "version"))
#saveout
write.csv(dfaALLpluspars, "dfaALLpluspars.csv")

```










# DESCRIPTIVES

Datasets for the steps below:

- accparsdfamerged.csv for wide version for corr
- accparsdfalong.csv for aggregate values by version for models similar to aim 1
- dfaALLpluspars.csv for longer version of DFA for longit models

## Read in datasets created above to conduct steps below
```{r}

#set wd again
setwd("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES")

accparsdfamerged <- read.csv("accparsdfamerged.csv")
accparsdfamerged$X <- NULL
  
accparsdfalong <- read.csv("accparsdfalong.csv")
accparsdfalong$X <- NULL

dfaALLpluspars <- read.csv("dfaALLpluspars.csv")
dfaALLpluspars$X <- NULL 

dfaALLpluspars <- dfaALLpluspars %>% 
  rename(alpha = Scaling.Exponent)

```


## Correlations
```{r}

#correlations across all study variables
my_matrix <- accparsdfamerged[, c(2, 4:5, 10, 14, 15:23)]

#rename to make easier to read
my_matrix <- my_matrix %>% 
  rename(
age = cage, 
parentEd = highed,      
totPts_nonemo = totPtsLines,
totPts_socioemo = totPtsPics,  
punRate_nonemo = ApunLines,  
rewRate_nonemo = ArewLines, 
beta_nonemo = betaLines,    
punRate_socioemo = ApunPics,
rewRate_socioemo = ArewPics, 
beta_socioemo = betaPics, 
basePhysio = basealpha, 
avgPhysio_nonemo = linesavgalpha,
avgPhysio_socioemo = picsavgalpha
  )


#Correlations
my_cor <- cor(my_matrix,use="complete.obs")
# upper<-my_cor
# upper[upper.tri(my_cor)]<-""
# upper<-as.data.frame(upper)
# print(xtable(upper), type="html")

#Another way of looking at correlations
corrplot(my_cor, type="upper", order="hclust", tl.col="black", tl.srt=45)
corrplot(my_cor, method = "color")
col = colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(my_cor, method="shade", shade.col=NA, tl.col="black",
                          tl.srt=100, col=col(200), addCoef.col="black", 
                          cl.pos="n", type = "lower", number.cex=.65)


```

## Plots
```{r}

######################
#########DFA plots
DFAlongmerged <- read.csv("dfalongmerged.csv")
DFAlongmerged$X <- NULL

#actual order
ggplot(data=DFAlongmerged, aes(x=origorder, y=Scaling.Exponent, group=factor(ID), color=factor(ID)))+
      geom_line(aes(group=ID), alpha = 0.3, show.legend = FALSE) +  
	    theme_classic() +
  ylab("Alpha") +
  xlab("Order of presentation")
ggsave("raworder alpha.png")

#plot the above to make sure it worked; include line that separates transition from lines to pics
ggplot(data=DFAlongmerged, aes(x=versordernum1, y=Scaling.Exponent, group=factor(ID), color=factor(ID)))+
      geom_line(aes(group=ID), alpha = 0.3, show.legend = FALSE) +  
  geom_vline(xintercept = 4, color = "red", linetype = "dashed") +
	    theme_classic() +
  ylab(expression(~ alpha)) +
  xlab("Baseline(0) -> Non-emotional(1-4) -> Socioemotional(5-8)")
ggsave("linespicsorder alpha.png")

#filter by version
DFAlongmerged2 <- DFAlongmerged %>%
  filter(version=="lines" | version=="pics")

#raw trajectories by version
ggplot(data = DFAlongmerged2, aes(x = versordernum2, y = Scaling.Exponent, color = factor(ID))) +
  geom_line(aes(linetype = factor(version), alpha = .3), show.legend = FALSE) +  # Use different line types for different versions
  scale_x_continuous(breaks = 1:4) +  # Only show ticks for 1-4
  theme_classic() +  # Use a classic theme
  labs(x = "Run", y = "Alpha")  # Label the axes
ggsave("runversion alpha.png")

#indi plots by version
ggplot(data = DFAlongmerged2, aes(x = versordernum2, y = Scaling.Exponent, color = version)) +
  geom_line(show.legend = T) +  # Use different line types for different versions
  facet_wrap(~ ID) +  # Create separate panels for each ID
  scale_x_continuous(breaks = 1:4) +  # Only show ticks for 1-4
  theme_classic() +  # Use a classic theme
  labs(x = "Phase (1 and 3 = acquisition; 2 and 4 = reversal)", y = "Alpha") 
ggsave("alphaovertimebyID.png")

###########

#separate plots by version
ggplot(data = dfaALLpluspars, aes(x = versordernum2, y = alpha, group=factor(Subject), color=factor(Subject)))+
geom_line(aes(group=Subject), alpha = 0.3, show.legend = FALSE) +  
  geom_line(alpha = 0.3, show.legend = F) +  
facet_wrap(~ version, labeller = labeller(version = c("lines" = "Non-Emotional", "pics" = "Socioemotional")))+ 
 stat_summary(fun = "mean", geom = "line", show.legend = F, aes(group = version), size=1) +
  #geom_smooth(aes(group = version), method = "loess",show.legend = F)+
  #scale_x_continuous(breaks = 1:4) +  
  scale_x_continuous(labels = c("Acq", "Rev", "Acq", "Rev"))+
  theme_classic()+
          theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 15)) +
  #theme(axis.text.x = element_text(angle = 20))+
  xlab("Learning phase") +
  ylab(expression(~ alpha)) 
ggsave("versionfacetalpha.png")

#scatter between avg physio and cog params
ggplot(data = accparsdfalong, aes(x = Arew, y = avgalpha, group_by(version), color = version)) + 
  geom_jitter() + 
  geom_smooth(aes(fill = version), method = "lm") +
  scale_color_manual(values=alpha(c("#C39E5C", "#DA723C"), .9))+
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Reward learning rate') +
  ylab(expression(~ alpha)) +
  theme_classic() 
  ggsave("ArewAlphaScatter.png", width = 5, height = 4)
  
ggplot(data = accparsdfalong, aes(x = Apun, y = avgalpha, group_by(version), color = version)) + 
  geom_jitter() + 
  geom_smooth(aes(fill = version), method = "lm") +
  scale_color_manual(values=alpha(c("#C39E5C", "#DA723C"), .9))+
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Punishment learning rate') +
  ylab(expression(~ alpha)) +
  theme_classic() 
  ggsave("ApunAlphaScatter.png", width = 5, height = 4)
  
ggplot(data = accparsdfalong, aes(x = beta, y = avgalpha, group_by(version), color = version)) + 
  geom_jitter() + 
  geom_smooth(aes(fill = version), method = "lm") +
  scale_color_manual(values=alpha(c("#C39E5C", "#DA723C"), .9))+
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Inverse temperature') +
  ylab(expression(~ alpha)) +
  theme_classic() 
  ggsave("betaAlphaScatter.png", width = 5, height = 4)

  
  
#avg physio density by version
ggdensity(accparsdfalong, x = "avgalpha", 
          add = "mean", rug = TRUE,
          color = "version", fill = "version",  
          xlab=expression(~ alpha),
          palette = c("#009E73", "#0072B2")) +
  scale_color_manual(values = c("#009E73", "#0072B2"), labels = c("lines" = "Non-Emotional", "pics" = "Socioemotional"))  +
  scale_fill_manual(values = c("#009E73", "#0072B2"), labels = c("lines" = "Non-Emotional", "pics" = "Socioemotional"))
ggsave("avgalphadensity.png", width = 4, height = 4)




##relations with totpts
ggplot(data = accparsdfalong, aes(x = avgalpha, y = totPts, group_by(version), color = version)) + 
  geom_point() + 
  geom_smooth(aes(fill = version), method = "lm") +
  scale_color_manual(values=alpha(c("#C39E5C", "#DA723C"), .9))+
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab(expression(~ alpha)) +
  ylab("Total points") +
  theme_classic() 
  ggsave("totPtsAlphaScatter.png", width = 5, height = 4)
  
ggplot(data = accparsdfalong, aes(x = cage, y = avgalpha, group_by(version), color = version)) + 
  geom_point() + 
  geom_smooth(aes(fill = version), method = "lm") +
  scale_color_manual(values=alpha(c("#C39E5C", "#DA723C"), .9))+
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab("Age") +
  ylab(expression(~ alpha)) +
  theme_classic() 
  ggsave("AgeAlphaScatter.png", width = 5, height = 4)
  
  
  
#color by total points
accparsdfamerged <- read_csv("accparsdfamerged.csv")

#lines
  ggplot(data = accparsdfamerged, aes(x = ApunLines, y = linesavgalpha, 
                                 color=totPtsLines)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Punishment Learning Rate') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("linespunalphaTOTPTSplot.png")

ggplot(data = accparsdfamerged, aes(x = ArewLines, y = linesavgalpha, 
                                 color=totPtsLines)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Reward Learning Rate') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("linesrewalphaTOTPTSplot.png")

ggplot(data = accparsdfamerged, aes(x = betaLines, y = linesavgalpha, 
                                 color=totPtsLines)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Inverse Temperature') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("linesbetaalphaTOTPTSplot.png")


#pics
  ggplot(data = accparsdfamerged, aes(x = ApunPics, y = picsavgalpha, 
                                 color=totPtsPics)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Punishment Learning Rate') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("picspunalphaTOTPTSplot.png")

ggplot(data = accparsdfamerged, aes(x = ArewPics, y = picsavgalpha, 
                                 color=totPtsPics)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Reward Learning Rate') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("picsrewalphaTOTPTSplot.png")

ggplot(data = accparsdfamerged, aes(x = betaPics, y = picsavgalpha, 
                                 color=totPtsPics)) + 
  geom_point(size=4, alpha= .7) + 
  geom_smooth(method=lm, color="black") +
  scale_color_gradient(low = "blue", high = "red") +
  ylab(expression(~ alpha)) +
  xlab ('Inverse Temperature') +
  theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15)
    ,
    axis.text.y = element_text(size = 15)) +
  labs(color = "Total points")
ggsave("picsbetaalphaTOTPTSplot.png")


```

# BRMS MODELS ON AVG LEVELS

## Setting up brms
```{r}

setwd("~/Library/CloudStorage/Box-Box/BSL General/Projects/LEAP/4_DATA ANALYSIS/AIM 2 LEAP ANALYSIS FILES")

accparsdfalong <-read_csv("accparsdfalong.csv")
accparsdfalong$X1 <- NULL

accparsdfalong$version <- as.factor(accparsdfalong$version)
#make vars centered
mean(accparsdfalong$cage, na.r=T) #13.27589
mean(accparsdfalong$Apun, na.r=T) #0.4789215
mean(accparsdfalong$Arew, na.r=T) #0.6718672
mean(accparsdfalong$beta, na.r=T) #2.136919

accparsdfalong <- accparsdfalong %>% 
  mutate (cage_c = cage - 13.27589,
          Apun_c = Apun - 0.4789215,
          Arew_c = Arew - 0.6718672,
          beta_c = beta - 2.136919)

#install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
library(posterior)
library(bayesplot)
color_scheme_set("brightblue")
check_cmdstan_toolchain()

#install_cmdstan(cores = 2, overwrite = T)

cmdstan_path()
cmdstan_version()

```

## Version only model
```{r}

##############
#Setup
test <- lmer(avgalpha ~ 1 + version + (1|Subject), data = accparsdfalong, REML = FALSE, control=lmerControl(optimizer = "Nelder_Mead"))
test <- lm(avgalpha ~ 1 + version, data = accparsdfalong)
summary(test)

#using similar sd and sigma priors as comp parameters given similar range in outcome and likelihood of individual diff
#mean of .85 reflects that I think most youth will fall in the middle ground for pink noise given the TD high SES sample I have; however I added a wide variance around this to account for some uncertainty
plotInvGamma(3,.6)
plotInvGamma(3,.1)

set.seed(5)
Alpha_f1 <- bf(avgalpha ~ 1 + version + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f1, data = accparsdfalong, family = student)

student_priors1 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m1_prior <- 
  brm(
    Alpha_f1,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors1,
    file = "Alpha_student_m1_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m1_prior, ndraws=100)


#############
#Run model 1

Alpha_student_m1 <- 
  brm(
    Alpha_f1,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors1,
    file = "Alpha_student_m1",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m1)

#Summarize model
summary(Alpha_student_m1)

#Test my hypothesis that youth will perform slightly better in socioemo:
hypothesis(Alpha_student_m1, "versionpics > 0") 

#Is there support for domain-general learning performance?
hypothesis(Alpha_student_m1, "versionpics = 0")

#Is there support for the hypothesis that youth perform better on lines?
hypothesis(Alpha_student_m1, "versionpics < 0")

#for individual subjects:
hypothesis(Alpha_student_m1, "versionpics > 0", group = "Subject", scope="coef") #56/56 showed greater performance on pics


################
#Posterior checks

plot(conditional_effects(Alpha_student_m1), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m1, ndraws=100)
pp_check(Alpha_student_m1, type = "intervals_grouped", group = "version")

#Sample from posterior
Posterior_student_m1_Alpha <- as_draws_df(Alpha_student_m1)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()
ggsave("Alpham1intercept.png", width = 4, height = 4)

#Plot the prior-posterior update plot for b:
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(prior_b), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b') +
  theme_classic()
ggsave("Alpham1beta.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__versionpics), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__versionpics), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  xlim(0,2)+
  theme_classic()
ggsave("Alpham1sd.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  xlim(0,2)+
  theme_classic()
ggsave("Alpham1sigma.png", width = 4, height = 4)

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()
ggsave("Alpham1corr.png", width = 4, height = 4)

#nu
ggplot(Posterior_student_m1_Alpha) +
  geom_density(aes(prior_nu), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(nu), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('nu') +
  theme_classic()
ggsave("Alpham1nu.png", width = 4, height = 4)

```

## Version + cog param model
```{r}

##############
#Setup


set.seed(5)
Alpha_f2 <- bf(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f2, data = accparsdfalong, family = student)

student_priors2 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=Apun_c), 
  prior(normal(0, 1), class = b, coef=Arew_c), 
  prior(normal(0, 1), class = b, coef=beta_c), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m2_prior <- 
  brm(
    Alpha_f2,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors2,
    file = "Alpha_student_m2_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m2_prior, ndraws=100)


#############
#Run model 2

Alpha_student_m2 <- 
  brm(
    Alpha_f2,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors2,
    file = "Alpha_student_m2",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m2)

#Summarize model
summary(Alpha_student_m2)

#Test my hypothesis that youth will perform slightly better in socioemo:
hypothesis(Alpha_student_m2, "versionpics > 0") 

#Is there support for domain-general learning performance?
hypothesis(Alpha_student_m2, "versionpics = 0")

#Is there support for the hypothesis that youth perform better on lines?
hypothesis(Alpha_student_m2, "versionpics < 0")

#for individual subjects:
hypothesis(Alpha_student_m2, "versionpics > 0", group = "Subject", scope="coef") #56/56 showed greater performance on pics


################
#Posterior checks

plot(conditional_effects(Alpha_student_m2), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m2, ndraws=100)
pp_check(Alpha_student_m2, type = "intervals_grouped", group = "version")

#Sample from posterior
Posterior_student_m2_Alpha <- as_draws_df(Alpha_student_m2)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()
ggsave("Alpham2intercept.png", width = 4, height = 4)

#Plot the prior-posterior update plot for b:
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(prior_b), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b') +
  theme_classic()
ggsave("Alpham2beta_c.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__versionpics), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__versionpics), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  xlim(0,1)+
  theme_classic()
ggsave("Alpham2sd.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  xlim(0,1)+
  theme_classic()
ggsave("Alpham2sigma.png", width = 4, height = 4)

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()
ggsave("Alpham2corr.png", width = 4, height = 4)

#nu
ggplot(Posterior_student_m2_Alpha) +
  geom_density(aes(prior_nu), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(nu), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('nu') +
  theme_classic()
ggsave("Alpham2nu.png", width = 4, height = 4)


```

The beta_c relation looks slightly quadratic, so I am going to test a quad for beta_c main effect and compare before moving onto interaction model

## Quadratic beta_c model
```{r}

##############
#Setup

set.seed(5)
Alpha_f3 <- bf(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + I(beta_c^2) + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f3, data = accparsdfalong, family = student)

student_priors3 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=Apun_c), 
  prior(normal(0, 1), class = b, coef=Arew_c), 
  prior(normal(0, 1), class = b, coef=beta_c), 
  prior(normal(0, 1), class = b, Ibeta_cE2), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m3_prior <- 
  brm(
    Alpha_f3,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors3,
    file = "Alpha_student_m3_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m3_prior, ndraws=100)


#############
#Run model 2

Alpha_student_m3 <- 
  brm(
    Alpha_f3,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors3,
    file = "Alpha_student_m3",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m3)

#Summarize model
summary(Alpha_student_m3)


################
#Posterior checks

plot(conditional_effects(Alpha_student_m3), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m3, ndraws=100)
pp_check(Alpha_student_m3, type = "intervals_grouped", group = "version")

#hyp tests

#version
hypothesis(Alpha_student_m3, "versionpics > 0") 
hypothesis(Alpha_student_m3, "versionpics = 0")
hypothesis(Alpha_student_m3, "versionpics < 0")

#Apun_c avg est was neg
hypothesis(Alpha_student_m3, "Apun_c < 0") 

#Arew_c avg est was pos
hypothesis(Alpha_student_m3, "Arew_c < 0") 

#beta_c int avg was pos
hypothesis(Alpha_student_m3, "beta_c > 0") 

#beta_c quad change avg was neg
hypothesis(Alpha_student_m3, "Ibeta_cE2 < 0") 




#Sample from posterior
Posterior_student_m3_Alpha <- as_draws_df(Alpha_student_m3)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()
ggsave("Alpham3intercept.png", width = 4, height = 4)

#Plot the prior-posterior update plot for b:
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_b_versionpics), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (version)') +
  theme_classic()
ggsave("Alpham3betavers.png", width = 4, height = 4)

ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_b_Apun_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Apun_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (pun rate)') +
  theme_classic()
ggsave("Alpham3betapun.png", width = 4, height = 4)

ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_b_Arew_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Arew_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (rew rate)') +
  theme_classic()
ggsave("Alpham3betarew.png", width = 4, height = 4)

ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_b_beta_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_beta_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (temp)') +
  theme_classic()
ggsave("Alpham3betatemp.png", width = 4, height = 4)


#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__versionpics), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__versionpics), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  xlim(0,2)+
  theme_classic()
ggsave("Alpham3sd.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  xlim(0,2)+
  theme_classic()
ggsave("Alpham1sigma.png", width = 4, height = 4)

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()
ggsave("Alpham3corr.png", width = 4, height = 4)

#nu
ggplot(Posterior_student_m3_Alpha) +
  geom_density(aes(prior_nu), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(nu), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('nu') +
  theme_classic()
ggsave("Alpham1nu.png", width = 4, height = 4)



```

## Compare models 
```{r}
#waic
waic(Alpha_student_m1) #-182.8
waic(Alpha_student_m2) #-202.8
waic(Alpha_student_m3) #-201.8

#LOOIC
Alpha_student_m1_loo <- loo(Alpha_student_m1, moment_match = TRUE) #-158.1
Alpha_student_m2_loo <- loo(Alpha_student_m2, moment_match = TRUE) #-174.3
Alpha_student_m3_loo <- loo(Alpha_student_m3, moment_match = TRUE) #-173.7

loo_compare(Alpha_student_m1_loo,Alpha_student_m2_loo,Alpha_student_m3_loo)

summary(Alpha_student_m1)
summary(Alpha_student_m2)
summary(Alpha_student_m3)

```

Model w/ linear beta_c model best (m2). Now seeing if we need age before testing interactions.

## Age as covariate
```{r}

##############
#Setup
test <- lmer(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + cage_c + (1|Subject), data = accparsdfalong, REML = FALSE, control=lmerControl(optimizer = "Nelder_Mead"))
summary(test)

set.seed(5)
Alpha_f4 <- bf(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + cage_c + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f4, data = accparsdfalong, family = student)

student_priors4 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=Apun_c), 
  prior(normal(0, 1), class = b, coef=Arew_c), 
  prior(normal(0, 1), class = b, coef=beta_c), 
  prior(normal(0, 1), class = b, cage_c), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m4_prior <- 
  brm(
    Alpha_f4,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors4,
    file = "Alpha_student_m4_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m4_prior, ndraws=100)


#############
#Run model 4

Alpha_student_m4 <- 
  brm(
    Alpha_f4,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors4,
    file = "Alpha_student_m4",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m4)

#Summarize model
summary(Alpha_student_m4)

hypothesis(Alpha_student_m4, "versionpics > 0") 
hypothesis(Alpha_student_m4, "versionpics < 0") 
hypothesis(Alpha_student_m4, "Arew_c > 0") 
2*log(1713.3)

hypothesis(Alpha_student_m4, "Apun_c < 0") 
hypothesis(Alpha_student_m4, "beta_c > 0") 
hypothesis(Alpha_student_m4, "cage_c > 0") 


################
#Posterior checks

plot(conditional_effects(Alpha_student_m4), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m4, ndraws=100)
pp_check(Alpha_student_m4, type = "intervals_grouped", group = "version")

#Sample from posterior
Posterior_student_m4_Alpha <- as_draws_df(Alpha_student_m4)

#Plot the prior-posterior update plot for the intercept:
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_Intercept), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()
ggsave("Alpham4intercept.png", width = 4, height = 4)

#Plot the prior-posterior update plot for b:
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_b_versionpics), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (versionpics)') +
  theme_classic()
ggsave("Alpham4bversion.png", width = 4, height = 4)

ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_b_Arew_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Arew_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (Arew)') +
  theme_classic()
ggsave("Alpham4bArew_c.png", width = 4, height = 4)

ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_b_Apun_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_Apun_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (Apun)') +
  theme_classic()
ggsave("Alpham4bApun_c.png", width = 4, height = 4)

ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_b_beta_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_beta_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (beta)') +
  theme_classic()
ggsave("Alpham4bbeta_c.png", width = 4, height = 4)

ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_b_cage_c), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(b_cage_c), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('b (cage)') +
  theme_classic()
ggsave("Alpham4bcage_c.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sd of intercepts and slopes:
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(sd_Subject__Intercept), fill="#FC4E07", color="black",alpha=0.3) + 
  geom_density(aes(sd_Subject__versionpics), fill="#228B22", color="black",alpha=0.4) + 
  geom_density(aes(prior_sd_Subject__versionpics), fill="steelblue", color="black",alpha=0.6) +
  xlab('sd') +
  xlim(0,1)+
  theme_classic()
ggsave("Alpham4sd.png", width = 4, height = 4)

#Plot the prior-posterior update plot for sigma:
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_sigma), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('sigma') +
  xlim(0,1)+
  theme_classic()
ggsave("Alpham4sigma.png", width = 4, height = 4)

#Plot the prior-posterior update plot for the correlation between varying intercepts and slopes:
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_cor_Subject), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(cor_Subject__Intercept__versionpics), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('cor') +
  theme_classic()
ggsave("Alpham4corr.png", width = 4, height = 4)

#nu
ggplot(Posterior_student_m4_Alpha) +
  geom_density(aes(prior_nu), fill="steelblue", color="black",alpha=0.6) +
  geom_density(aes(nu), fill="#FC4E07", color="black",alpha=0.6) + 
  xlab('nu') +
  theme_classic()
ggsave("Alpham4nu.png", width = 4, height = 4)

```

## Model comparisons
```{r}

#main effects vs model with age

#waic
waic(Alpha_student_m2) #-202.8
waic(Alpha_student_m4) #-203.3

#LOOIC
Alpha_student_m2_loo #-174.3
Alpha_student_m4_loo <- loo(Alpha_student_m4, moment_match = TRUE) #-175.8

loo_compare(Alpha_student_m1_loo,Alpha_student_m2_loo,Alpha_student_m3_loo, Alpha_student_m4_loo)

```

Age model 4 slightly better.

## Interaction model
```{r}

##############
#Setup

set.seed(5)
Alpha_f5 <- bf(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + cage_c + Apun_c:version + Arew_c:version + beta_c:version + cage_c:version + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f5, data = accparsdfalong, family = student)

student_priors5 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=Apun_c), 
  prior(normal(0, 1), class = b, coef=Arew_c), 
  prior(normal(0, 1), class = b, coef=beta_c), 
  prior(normal(0, 1), class = b, cage_c), 
  prior(normal(0, 1), class = b, coef=versionpics:Apun_c), 
  prior(normal(0, 1), class = b, coef=versionpics:Arew_c), 
  prior(normal(0, 1), class = b, coef=versionpics:beta_c), 
  prior(normal(0, 1), class = b, coef=versionpics:cage_c), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m5_prior <- 
  brm(
    Alpha_f5,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors5,
    file = "Alpha_student_m5_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m5_prior, ndraws=100)


#############
#Run model 2

Alpha_student_m5 <- 
  brm(
    Alpha_f5,
    data = accparsdfalong,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors5,
    file = "Alpha_student_m5",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m5)

#Summarize model
summary(Alpha_student_m5)

#Test my hypothesis that youth will perform slightly better in socioemo:
hypothesis(Alpha_student_m5, "versionpics > 0") 

#Is there support for domain-general learning performance?
hypothesis(Alpha_student_m5, "versionpics = 0")

#Is there support for the hypothesis that youth perform better on lines?
hypothesis(Alpha_student_m5, "versionpics < 0")

#for individual subjects:
hypothesis(Alpha_student_m5, "versionpics > 0", group = "Subject", scope="coef") #56/56 showed greater performance on pics


################
#Posterior checks

plot(conditional_effects(Alpha_student_m5), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m5, ndraws=100)
pp_check(Alpha_student_m5, type = "intervals_grouped", group = "version")

#Sample from posterior
Posterior_student_m5_Alpha <- as_draws_df(Alpha_student_m3)


```

## Model comparisons
```{r}

#main effects vs interaction model

#waic
waic(Alpha_student_m1) #-182.8
waic(Alpha_student_m2) #-202.8
waic(Alpha_student_m3) #-201.8
waic(Alpha_student_m4) #-203.3
waic(Alpha_student_m5) #-195.9

#LOOIC
Alpha_student_m1_loo <- loo(Alpha_student_m1, moment_match = TRUE) #-158.1
Alpha_student_m2_loo <- loo(Alpha_student_m2, moment_match = TRUE) #-174.3
Alpha_student_m3_loo <- loo(Alpha_student_m3, moment_match = TRUE) #-173.7
Alpha_student_m4_loo <- loo(Alpha_student_m4, moment_match = TRUE) #-175.8
Alpha_student_m5_loo <- loo(Alpha_student_m5, moment_match = TRUE) #-166.5

loo_compare(Alpha_student_m1_loo,Alpha_student_m2_loo,Alpha_student_m3_loo, Alpha_student_m4_loo, Alpha_student_m5_loo)

```

Interaction model not better, so main effects model (m4) is the final.


## Plot final model
```{r}

#summarize again
summary(Alpha_student_m4)
plot(conditional_effects(Alpha_student_m4), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#conditional plots
gg <- plot(conditional_effects(Alpha_student_m3),
     palette = "virdis",
     points = T, 
     theme = theme_classic())

p1 <- gg[[1]]
p2 <- gg[[2]]
p3 <- gg[[3]]

p3 <- p3 + guides(fill = "none")

gg <- p3 + 
  labs(x = "Reward Learning Rate", y = expression(~alpha))

gg <- gg + theme(axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15))

#figure out which effects have sufficient support
hypothesis(Alpha_student_m4, "versionpics > 0") #73%PP
hypothesis(Alpha_student_m4, "Apun_c < 0")#70%PP
hypothesis(Alpha_student_m4, "Arew_c > 0")#100%PP, so will plot this
hypothesis(Alpha_student_m4, "beta_c >0")#84%
#only Arew_c sig, 





####detailed plots


# Generate predictions for all observations
predictions <- predict(Alpha_student_m4, newdata = accparsdfalong, allow_new_levels = TRUE)

# Add the predicted values as a new column in the original dataset
accparsdfalong$predicted_yield <- predictions
colnames(accparsdfalong)
accparsdfalong$yhat<-accparsdfalong$predicted_yield[,"Estimate"]


#version
#Extracted summary stats 
summary(Alpha_student_m4) #take uCI and lCI away from intercept to get mins for pics
means <- data.frame(version = c("lines", "pics"),
                    mean = c(0.84, 0.85),
                    ymin = c(0.79, 0.81), 
                    ymax = c(0.88, 0.89))
#Needs subject to merge with raw data
subjects <- data.frame(Subject = c("mean"))
means <- cbind(means, subjects)

#Make raw data plot for background
p <- ggplot(data = accparsdfalong, aes(x = version, y = avgalpha, group = Subject, color = as.factor(Subject))) +
  geom_violin(aes(x = version, y = avgalpha, group = version), fill = "gray", alpha = 0.5, linetype="blank") +
  geom_point(alpha = 0.5, size = 3.5) +
  geom_line(alpha = 0.5, size = 1) +
  theme_classic() +
  theme(legend.position="none") +
  ylab(expression(~ alpha)) +
  xlab ("Version") +
  scale_x_discrete(labels = c("Non-Emotional", "Socioemotional")) +
  theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15)
  )
  
# Then, add the mean points and error bars to the scatterplot as additional layers
m3Alphaversplot <- p + geom_point(data = means, aes(x = version, y = mean, group = Subject), color = "black", size = 5) +
  geom_line(data = means, aes(x = version, y = mean, group = Subject), color = "black", size =1.5 ) +
  geom_errorbar(data = means, aes(x = version, y = mean, ymin = ymin, ymax = ymax), color = "black", width = 0.2) 

#save
ggsave("m4Alphaversplot.png", width = 7, height = 5.5)




#Apun
ggplot(data = accparsdfalong, aes(x = Apun, y = yhat, group_by(version), color = version)) + 
  geom_point(aes(y=avgalpha), size = 4, alpha= .7) + 
  geom_smooth(aes(group = 1), method = "lm", size = 2, color = "black") +
   scale_color_manual(values=alpha(c("#009E73", "#0072B2"), .9), labels = c("Non-Emotional", "Socioemotional"), guide="none") +
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Punishment Learning Rate') +
  ylab(expression(~ alpha)) +
theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15)) 
ggsave("m4Apun_alpha_scatter.png", width = 7, height = 5.5)

#arew
ggplot(data = accparsdfalong, aes(x = Arew, y = yhat, group_by(version), color = version)) + 
  geom_point(aes(y=avgalpha), size = 4, alpha= .7) + 
  geom_smooth(aes(group = 1), method = "lm", size = 2, color = "black") +
   scale_color_manual(values=alpha(c("#009E73", "#0072B2"), .9), labels = c("Non-Emotional", "Socioemotional"), guide = "none") +
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Reward Learning Rate') +
  ylab(expression(~ alpha)) +
theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15)) 
ggsave("m4Arew_alpha_scatter.png", width = 7, height = 5.5)

#beta
ggplot(data = accparsdfalong, aes(x = beta, y = yhat, group_by(version), color = version)) + 
  geom_point(aes(y=avgalpha), size = 4, alpha= .7) + 
  geom_smooth(aes(group = 1), method = "lm", size = 2, color = "black") +
   scale_color_manual(values=alpha(c("#009E73", "#0072B2"), .9), labels = c("Non-Emotional", "Socioemotional"), guide = "none") +
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Inverse Temperature') +
  ylab(expression(~ alpha)) +
theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15)) 
ggsave("m4beta_alpha_scatter.png", width = 7, height = 5.5)

#age
ggplot(data = accparsdfalong, aes(x = cage, y = yhat, group_by(version), color = version)) + 
  geom_point(aes(y=avgalpha), size = 4, alpha= .7) + 
  geom_smooth(aes(group = 1), method = "lm", size = 2, color = "black") +
   scale_color_manual(values=alpha(c("#009E73", "#0072B2"), .9), labels = c("Non-Emotional", "Socioemotional"), guide = "none") +
  scale_fill_manual(values=alpha(c("#C39E5C", "#DA723C"), .7))+
  xlab('Child Age') +
  ylab(expression(~ alpha)) +
theme_classic() +
        theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15)) 
ggsave("m4age_alpha_scatter.png", width = 7, height = 5.5)

```

## Effect sizes
```{r}

# In within-subject designs, Cohen's d is calculated as the difference in means between the two conditions divided by the pooled standard deviation of the differences. Using this method, the effect size estimate reflects the magnitude of the difference between the two conditions relative to the variability within the subjects, which is appropriate for a within-person design.

# Calculate Cohen's d for versionpics
summary(Alpha_student_m4)
# versionpics coef
version_diff <- 0.01 
# Compute pooled standard deviation of outcome
# Compute pooled standard deviation of outcome while ignoring NA
pooled_sd <- sqrt((sd(accparsdfalong$avgalpha[accparsdfalong$version == "lines" & !is.na(accparsdfalong$avgalpha)], na.rm = TRUE)^2 +
                 sd(accparsdfalong$avgalpha[accparsdfalong$version == "pics" & !is.na(accparsdfalong$avgalpha)], na.rm = TRUE)^2) / 2)

# Compute Cohen's d
cohens_d <- version_diff / pooled_sd
cohens_d #0.06596784


#Continuous effects
summary(Alpha_student_m4)

#arew
0.22*sd(accparsdfalong$Arew, na.rm=TRUE)/sd(accparsdfalong$avgalpha, na.rm=T)  #0.2861392
r <- cor(accparsdfalong$Arew, accparsdfalong$avgalpha, use = "complete.obs")
0.22/(sd(accparsdfalong$Arew, na.rm=TRUE) * sd(accparsdfalong$avgalpha, na.rm=T)) * sqrt(r^2)

#apun
-0.02*sd(accparsdfalong$Apun, na.rm=TRUE)/sd(accparsdfalong$avgalpha, na.rm=T) #-0.03268404
r <- cor(accparsdfalong$Apun, accparsdfalong$avgalpha, use = "complete.obs")
-0.02/(sd(accparsdfalong$Apun, na.rm=TRUE) * sd(accparsdfalong$avgalpha, na.rm=T)) * sqrt(0.02^2) #-0.01064411

#beta
0.02*sd(accparsdfalong$beta, na.rm=TRUE)/sd(accparsdfalong$avgalpha, na.rm=T) #0.0753706


#age
.04*sd(accparsdfalong$cage_c, na.rm=TRUE)/sd(accparsdfalong$avgalpha, na.rm=T) #0.2406289
r <- cor(accparsdfalong$cage_c, accparsdfalong$avgalpha, use = "complete.obs")
.04/(sd(accparsdfalong$cage_c, na.rm=TRUE) * sqrt(r^2)) #0.1690085


```


## Outlier sensitivity check
```{r}

#ID #46 had very low alpha, so going to re-run final model to see if results change

accparsdfalong2 <- accparsdfalong %>% 
  filter(Subject != 046)

set.seed(5)
Alpha_f4.2 <- bf(avgalpha ~ 1 + version + Apun_c + Arew_c + beta_c + cage_c + (1+version|Subject))

#Setting priors in a principled way
get_prior(Alpha_f4.2, data = accparsdfalong, family = student)

student_priors4.2 <- c(
  prior(normal(.85, .5), class = Intercept, lb=0, ub=2), 
  prior(inv_gamma(3,.6), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=Apun_c), 
  prior(normal(0, 1), class = b, coef=Arew_c), 
  prior(normal(0, 1), class = b, coef=beta_c), 
  prior(normal(0, 1), class = b, cage_c), 
  prior(inv_gamma(3,.6), class = sd, coef = versionpics, group = Subject),
  prior(inv_gamma(3,.7), class = sigma),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_m4.2_prior <- 
  brm(
    Alpha_f4.2,
    data = accparsdfalong2,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors4.2,
    file = "Alpha_student_m4.2_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_m4.2_prior, ndraws=100)


#############
#Run model 

Alpha_student_m4.2 <- 
  brm(
    Alpha_f4.2,
    data = accparsdfalong2,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priors4.2,
    file = "Alpha_student_m4.2",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_m4.2)

#Summarize model
summary(Alpha_student_m4.2)


################
#Posterior checks

plot(conditional_effects(Alpha_student_m4.2), 
     points = T, 
     theme = theme_minimal(), 
     palette = "magma") 

#pp check          
pp_check(Alpha_student_m4.2, ndraws=100)
pp_check(Alpha_student_m4.2, type = "intervals_grouped", group = "version")



```
























# LONGITUDINAL CHECKS

## Descriptives to determine whether longit model best
```{r}

#Examine means and sd of alpha for each time point; if stable, then might be best to stick with pre-reg analyses

mean(dfaALLpluspars$alpha, na.rm=T) #0.8375542
sd(dfaALLpluspars$alpha, na.rm=T)# 0.1917748

t1 <- dfaALLpluspars %>% 
  filter(versordernum2c == 0) 
mean(t1$alpha, na.rm=T) #.86
sd(t1$alpha, na.rm=T) #.18

t2 <- dfaALLpluspars %>% 
  filter(versordernum2c == 1) 
mean(t2$alpha, na.rm=T) #.82
sd(t2$alpha, na.rm=T) #.18

t3 <- dfaALLpluspars %>% 
  filter(versordernum2c == 2) 
mean(t3$alpha, na.rm=T) #.82
sd(t3$alpha, na.rm=T) #.18

t4 <- dfaALLpluspars %>% 
  filter(versordernum2c == 3) 
mean(t4$alpha, na.rm=T) #.84
sd(t4$alpha, na.rm=T) #.21

#pretty stable over time








#ICC

#Baseline model
Basealpha.lmer = lmer(Scaling.Exponent ~ 1 + (1|Subject), data = dfaALLpluspars, REML = FALSE)
summary(Basealpha.lmer) # Btwn + w/in variation: 0.01608 + 0.02042 = .0.0365
icc(Basealpha.lmer) # Adjusted ICC: 0.441 Conditional ICC: 0.441
plot(Basealpha.lmer) #assumptions 

#explore plot
ggplot(data = dfaALLpluspars, aes(x = versordernum2c, y = alpha, color=version)) +
  geom_smooth(method="lm", lwd = 1.5, show.legend = T) +
  geom_jitter(aes(y=alpha), alpha = 0.3, show.legend = FALSE) +
  theme_bw() +
  ylab("alpha") +
  xlab("run")

#combined, these descriptives suggest that time does not really play a meaningful role and reinforces using avg alpha scores

```

## Time and version model
```{r}

##############
#Setup

# > sd(accparsdfalong$avgalpha, na.rm=T)
# [1] 0.151643
# > mean(accparsdfalong$avgalpha, na.rm=T)
# [1] 0.8384007
dfaALLpluspars <- dfaALLpluspars %>% 
  rename(alpha = Scaling.Exponent)

set.seed(5)
Alpha_lf1 <- bf(alpha ~ 1 + versordernum2c + version + (1 + versordernum2c|Subject/version))

#Setting priors in a principled way
get_prior(Alpha_lf1, data = dfaALLpluspars, family = student)

student_priorsl1 <- c(
  prior(cauchy(.85, .3), class = Intercept, lb=0, ub=2), 
  prior(normal(.15, .15), class = sd, coef = Intercept, group = Subject),
  prior(normal(0, 1), class = sd, coef = versordernum2c, group = Subject),
  prior(normal(0, 1), class = b, coef=versionpics), 
  prior(normal(0, 1), class = b, coef=versordernum2c), 
  prior(normal(.3, .15), class = sigma, lb=0, ub=1),
  prior(lkj(2), class = cor),
  prior(gamma(2,.1), class = nu)) 


Alpha_student_lm1_prior <- 
  brm(
    Alpha_lf1,
    data = dfaALLpluspars,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priorsl1,
    file = "Alpha_student_lm1_prior",
    #refit = "on_change",
    sample_prior = "only", #only samples from the priors
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
#this samples from the prior dist (joint parameter space) to see the full uncertainty of our predictions
#darker line helps to establish the scale of the outcomes
#moves from possible space of parameter values -- uses that space to optimize 
pp_check(Alpha_student_lm1_prior, ndraws=100)



#############
#Run model 1

Alpha_student_lm1 <- 
  brm(
    Alpha_lf1,
    data = dfaALLpluspars,
    save_pars = save_pars(all = TRUE),
    family = student,
    prior = student_priorsl1,
    file = "Alpha_student_lm1",
    #refit = "on_change",
    sample_prior = T,
    iter = 4000, 
    warmup = 1000,
    cores = 4,
    chains = 4,
    backend = "cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

#Quick convergence check
plot(Alpha_student_lm1)

#Summarize model
summary(Alpha_student_lm1)

#plot
plot(conditional_effects(Alpha_student_lm1),
     palette = "virdis",
     points = T, 
     theme = theme_classic())

#summary and plot show nothing much going on 


```

